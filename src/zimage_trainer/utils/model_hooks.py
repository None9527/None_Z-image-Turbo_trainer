# -*- coding: utf-8 -*-
"""
ğŸ”Œ Model Hooks - æ¨¡å‹ä¼˜åŒ–æŒ‚è½½æ¨¡å—

é€šè¿‡ PyTorch Hook æœºåˆ¶ä¸º diffusers æ¨¡å‹æ·»åŠ ä¼˜åŒ–åŠŸèƒ½ï¼Œ
æ— éœ€ä¿®æ”¹æ¨¡å‹æºç å³å¯å®ç°ï¼š
- Block Swapping (æ˜¾å­˜ä¼˜åŒ–)
- Attention Backend åˆ‡æ¢
- Gradient Checkpointing å¢å¼º

Usage:
    from diffusers import ZImageTransformer2DModel
    from zimage_trainer.utils.model_hooks import apply_block_swapper, apply_attention_optimization
    
    transformer = ZImageTransformer2DModel.from_pretrained(...)
    
    # æŒ‚è½½ Block Swapper
    apply_block_swapper(transformer, blocks_to_swap=8, device="cuda")
    
    # è®¾ç½® Attention Backend
    apply_attention_optimization(transformer, backend="flash")
"""

import torch
import torch.nn as nn
from typing import Optional, List, Callable, Any
import logging
import gc

logger = logging.getLogger(__name__)


class BlockSwapperHook:
    """
    é€šè¿‡ Hook å®ç°çš„ Block Swapper
    
    æ— éœ€ä¿®æ”¹æ¨¡å‹æºç ï¼Œé€šè¿‡ register_forward_pre_hook å’Œ register_forward_hook
    åœ¨æ¯å±‚å‰å‘ä¼ æ’­å‰/åè‡ªåŠ¨è¿›è¡Œ GPU/CPU äº¤æ¢ã€‚
    """
    
    def __init__(
        self,
        blocks_to_swap: int,
        device: torch.device,
        verbose: bool = True,
    ):
        self.blocks_to_swap = blocks_to_swap
        self.device = device
        self.cpu_device = torch.device("cpu")
        self.verbose = verbose
        
        self.layers: Optional[nn.ModuleList] = None
        self.n_layers: int = 0
        self.swap_start_idx: int = 0
        self.layer_on_gpu: List[bool] = []
        self.hooks: List[Any] = []
        
        # ç»Ÿè®¡
        self.swap_in_count = 0
        self.swap_out_count = 0
    
    def setup(self, layers: nn.ModuleList) -> "BlockSwapperHook":
        """
        è®¾ç½®è¦ç®¡ç†çš„å±‚å¹¶æ³¨å†Œ Hook
        
        Args:
            layers: Transformer çš„ layers (nn.ModuleList)
            
        Returns:
            self (æ”¯æŒé“¾å¼è°ƒç”¨)
        """
        self.layers = layers
        self.n_layers = len(layers)
        self.swap_start_idx = max(0, self.n_layers - self.blocks_to_swap)
        
        self.layer_on_gpu = [True] * self.n_layers
        
        if self.blocks_to_swap <= 0:
            if self.verbose:
                logger.info("[BlockSwapHook] ç¦ç”¨ (blocks_to_swap=0)")
            return self
        
        # å°†å N å±‚ç§»åˆ° CPU
        layers_moved = 0
        total_params = 0
        
        for i in range(self.swap_start_idx, self.n_layers):
            layer = self.layers[i]
            layer_params = sum(p.numel() for p in layer.parameters())
            total_params += layer_params
            layer.to(self.cpu_device)
            self.layer_on_gpu[i] = False
            layers_moved += 1
        
        # æ³¨å†Œ Hook
        for i, layer in enumerate(self.layers):
            # pre_hook: å°†å±‚ç§»åˆ° GPU
            pre_hook = layer.register_forward_pre_hook(
                self._make_pre_hook(i)
            )
            # post_hook: å°†å±‚ç§»å› CPU (å¦‚æœåœ¨äº¤æ¢èŒƒå›´å†…)
            post_hook = layer.register_forward_hook(
                self._make_post_hook(i)
            )
            self.hooks.extend([pre_hook, post_hook])
        
        # æ¸…ç† GPU ç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        if self.verbose:
            param_mb = total_params * 2 / (1024 * 1024)  # BF16/FP16
            logger.info(f"[BlockSwapHook] å·²å°† {layers_moved} å±‚ç§»åˆ° CPU")
            logger.info(f"[BlockSwapHook] äº¤æ¢èŒƒå›´: layer[{self.swap_start_idx}] ~ layer[{self.n_layers-1}]")
            logger.info(f"[BlockSwapHook] é¢„è®¡èŠ‚çœæ˜¾å­˜: ~{param_mb:.1f} MB")
            logger.info(f"[BlockSwapHook] å·²æ³¨å†Œ {len(self.hooks)} ä¸ª Hook")
        
        return self
    
    def _make_pre_hook(self, layer_idx: int) -> Callable:
        """åˆ›å»º pre_hook (swap_in)"""
        def hook(module, inputs):
            if self.blocks_to_swap <= 0:
                return
            if not self.layer_on_gpu[layer_idx]:
                module.to(self.device)
                self.layer_on_gpu[layer_idx] = True
                self.swap_in_count += 1
        return hook
    
    def _make_post_hook(self, layer_idx: int) -> Callable:
        """åˆ›å»º post_hook (swap_out)"""
        def hook(module, inputs, outputs):
            if self.blocks_to_swap <= 0:
                return
            if layer_idx >= self.swap_start_idx and self.layer_on_gpu[layer_idx]:
                module.to(self.cpu_device)
                self.layer_on_gpu[layer_idx] = False
                self.swap_out_count += 1
        return hook
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰ Hook"""
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()
        logger.info("[BlockSwapHook] å·²ç§»é™¤æ‰€æœ‰ Hook")
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "blocks_to_swap": self.blocks_to_swap,
            "n_layers": self.n_layers,
            "swap_start_idx": self.swap_start_idx,
            "swap_in_count": self.swap_in_count,
            "swap_out_count": self.swap_out_count,
            "layers_on_gpu": sum(self.layer_on_gpu),
            "layers_on_cpu": self.n_layers - sum(self.layer_on_gpu),
        }


def apply_block_swapper(
    transformer: nn.Module,
    blocks_to_swap: int,
    device: Optional[torch.device] = None,
    verbose: bool = True,
) -> Optional[BlockSwapperHook]:
    """
    ä¸º transformer æ¨¡å‹æŒ‚è½½ Block Swapper (é€šè¿‡ Hook)
    
    Args:
        transformer: ZImageTransformer2DModel æˆ–å…¶ä»–æœ‰ .layers å±æ€§çš„æ¨¡å‹
        blocks_to_swap: è¦äº¤æ¢åˆ° CPU çš„å±‚æ•°ï¼Œ0 è¡¨ç¤ºç¦ç”¨
        device: GPU è®¾å¤‡ï¼ŒNone æ—¶è‡ªåŠ¨æ£€æµ‹
        verbose: æ˜¯å¦æ‰“å°æ—¥å¿—
        
    Returns:
        BlockSwapperHook å®ä¾‹ï¼Œå¦‚æœç¦ç”¨åˆ™è¿”å› None
        
    Example:
        >>> from diffusers import ZImageTransformer2DModel
        >>> transformer = ZImageTransformer2DModel.from_pretrained(...)
        >>> swapper = apply_block_swapper(transformer, blocks_to_swap=8)
        >>> # è®­ç»ƒå®Œæˆåç§»é™¤
        >>> swapper.remove_hooks()
    """
    if blocks_to_swap <= 0:
        logger.info("[BlockSwapper] ç¦ç”¨ (blocks_to_swap=0)")
        return None
    
    # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
    if device is None:
        device = next(transformer.parameters()).device
    
    # æŸ¥æ‰¾ layers
    if hasattr(transformer, "layers"):
        layers = transformer.layers
    elif hasattr(transformer, "transformer_blocks"):
        layers = transformer.transformer_blocks
    else:
        logger.warning("[BlockSwapper] æœªæ‰¾åˆ° .layers æˆ– .transformer_blocksï¼Œè·³è¿‡")
        return None
    
    # åˆ›å»ºå¹¶è®¾ç½® Hook
    swapper = BlockSwapperHook(
        blocks_to_swap=blocks_to_swap,
        device=device,
        verbose=verbose,
    )
    swapper.setup(layers)
    
    return swapper


def apply_attention_optimization(
    transformer: nn.Module,
    backend: str = "flash",
) -> bool:
    """
    è®¾ç½® transformer çš„ attention backend
    
    diffusers å†…ç½®æ”¯æŒçš„ backend:
    - "flash": Flash Attention 2
    - "_flash_3": Flash Attention 3 (å¦‚æœå¯ç”¨)
    - "xformers": xformers memory_efficient_attention
    - "sdpa": PyTorch SDPA
    - None: é»˜è®¤åç«¯
    
    Args:
        transformer: ZImageTransformer2DModel ç­‰
        backend: åç«¯åç§°
        
    Returns:
        æ˜¯å¦æˆåŠŸè®¾ç½®
        
    Example:
        >>> apply_attention_optimization(transformer, backend="flash")
    """
    # æ–¹æ³•1: diffusers å†…ç½® API
    if hasattr(transformer, "set_attention_backend"):
        try:
            transformer.set_attention_backend(backend)
            logger.info(f"[Attention] å·²è®¾ç½®åç«¯: {backend}")
            return True
        except Exception as e:
            logger.warning(f"[Attention] set_attention_backend å¤±è´¥: {e}")
    
    # æ–¹æ³•2: è®¾ç½®ç±»å˜é‡ _attention_backend
    set_count = 0
    for name, module in transformer.named_modules():
        if hasattr(module, "_attention_backend"):
            module._attention_backend = backend
            set_count += 1
    
    if set_count > 0:
        logger.info(f"[Attention] å·²ä¸º {set_count} ä¸ªæ¨¡å—è®¾ç½®åç«¯: {backend}")
        return True
    
    # æ–¹æ³•3: enable_xformers_memory_efficient_attention (æ—§ API)
    if backend == "xformers" and hasattr(transformer, "enable_xformers_memory_efficient_attention"):
        try:
            transformer.enable_xformers_memory_efficient_attention()
            logger.info("[Attention] å·²å¯ç”¨ xformers")
            return True
        except Exception as e:
            logger.warning(f"[Attention] xformers å¯ç”¨å¤±è´¥: {e}")
    
    logger.warning(f"[Attention] æ¨¡å‹ä¸æ”¯æŒ attention backend è®¾ç½®")
    return False


def enable_gradient_checkpointing(
    transformer: nn.Module,
    use_reentrant: bool = False,
) -> bool:
    """
    å¯ç”¨ gradient checkpointing
    
    Args:
        transformer: æ¨¡å‹
        use_reentrant: æ˜¯å¦ä½¿ç”¨ reentrant æ¨¡å¼ (å»ºè®® False)
        
    Returns:
        æ˜¯å¦æˆåŠŸå¯ç”¨
    """
    # diffusers æ¨¡å‹å†…ç½®æ–¹æ³•
    if hasattr(transformer, "enable_gradient_checkpointing"):
        transformer.enable_gradient_checkpointing()
        logger.info("[GradCkpt] å·²å¯ç”¨ gradient checkpointing")
        return True
    
    # é€šç”¨æ–¹æ³•
    if hasattr(transformer, "gradient_checkpointing"):
        transformer.gradient_checkpointing = True
        logger.info("[GradCkpt] å·²è®¾ç½® gradient_checkpointing = True")
        return True
    
    logger.warning("[GradCkpt] æ¨¡å‹ä¸æ”¯æŒ gradient checkpointing")
    return False


def apply_all_optimizations(
    transformer: nn.Module,
    blocks_to_swap: int = 0,
    attention_backend: str = "flash",
    gradient_checkpointing: bool = True,
    device: Optional[torch.device] = None,
    verbose: bool = True,
) -> dict:
    """
    ä¸€é”®åº”ç”¨æ‰€æœ‰ä¼˜åŒ–
    
    Args:
        transformer: æ¨¡å‹
        blocks_to_swap: Block Swap å±‚æ•° (0 ç¦ç”¨)
        attention_backend: Attention åç«¯
        gradient_checkpointing: æ˜¯å¦å¯ç”¨ gradient checkpointing
        device: GPU è®¾å¤‡
        verbose: æ˜¯å¦æ‰“å°æ—¥å¿—
        
    Returns:
        åŒ…å«å„ä¼˜åŒ–ç»“æœçš„å­—å…¸
        
    Example:
        >>> from diffusers import ZImageTransformer2DModel
        >>> transformer = ZImageTransformer2DModel.from_pretrained(...)
        >>> results = apply_all_optimizations(
        ...     transformer,
        ...     blocks_to_swap=8,
        ...     attention_backend="flash",
        ...     gradient_checkpointing=True,
        ... )
    """
    results = {
        "block_swapper": None,
        "attention_backend": False,
        "gradient_checkpointing": False,
    }
    
    # 1. Gradient Checkpointing (å¿…é¡»åœ¨å…¶ä»–ä¼˜åŒ–å‰å¯ç”¨)
    if gradient_checkpointing:
        results["gradient_checkpointing"] = enable_gradient_checkpointing(transformer)
    
    # 2. Attention Backend
    if attention_backend:
        results["attention_backend"] = apply_attention_optimization(
            transformer, backend=attention_backend
        )
    
    # 3. Block Swapper
    if blocks_to_swap > 0:
        results["block_swapper"] = apply_block_swapper(
            transformer,
            blocks_to_swap=blocks_to_swap,
            device=device,
            verbose=verbose,
        )
    
    if verbose:
        logger.info("=" * 50)
        logger.info("[Optimizations] åº”ç”¨ç»“æœ:")
        logger.info(f"  - Gradient Checkpointing: {'âœ“' if results['gradient_checkpointing'] else 'âœ—'}")
        logger.info(f"  - Attention Backend: {'âœ“' if results['attention_backend'] else 'âœ—'}")
        logger.info(f"  - Block Swapper: {'âœ“' if results['block_swapper'] else 'âœ—'}")
        logger.info("=" * 50)
    
    return results
