#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Z-Image Latent Cache Script (Standalone - Multi-GPU Safe)

Áã¨Á´ãÁºìÂ≠òËÑöÊú¨ÔºåÈÅøÂÖçËß¶Âèë zimage_trainer/__init__.py ÂØºËá¥ CUDA ÊèêÂâçÂàùÂßãÂåñ„ÄÇ
‰ΩøÁî®Áã¨Á´ãÂ≠êËøõÁ®ãÂêØÂä®ÊñπÂºèÔºåÁ°Æ‰øù CUDA_VISIBLE_DEVICES Âú® torch ÂØºÂÖ•ÂâçËÆæÁΩÆ„ÄÇ

Usage:
    python scripts/cache_latents_standalone.py \
        --vae /path/to/vae \
        --input_dir /path/to/images \
        --output_dir /path/to/cache \
        --resolution 1024
"""

# === ÈáçË¶ÅÔºö‰∏çË¶ÅÂú®Ê®°ÂùóÈ°∂Â±ÇÂØºÂÖ•‰ªª‰ΩïÂèØËÉΩËß¶Âèë CUDA ÁöÑÂ∫ì ===
# torch, PIL, numpy, diffusers Á≠âÈÉΩÂøÖÈ°ªÂª∂ËøüÂØºÂÖ•

import argparse
import logging
import os
import sys
from pathlib import Path
from typing import List, Tuple

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# Z-Image architecture identifier
ARCHITECTURE = "zi"


def find_images(input_dir: str, extensions: Tuple[str, ...] = ('.jpg', '.jpeg', '.png', '.webp')) -> List[Path]:
    """Êü•ÊâæÁõÆÂΩï‰∏≠ÁöÑÊâÄÊúâÂõæÁâá (ÈÄíÂΩí)"""
    input_path = Path(input_dir)
    images = set()
    for ext in extensions:
        images.update(input_path.rglob(f'*{ext}'))
        images.update(input_path.rglob(f'*{ext.upper()}'))
    return sorted(list(images))


def run_single_gpu_worker(gpu_id: int, vae_path: str, input_dir: str, output_dir: str, 
                          resolution: int, skip_existing: bool, image_paths: List[str],
                          progress_queue):
    """
    Âçï‰∏™ GPU ÁöÑ worker ÂáΩÊï∞ÔºàÂú®Áã¨Á´ãËøõÁ®ã‰∏≠ËøêË°åÔºâ
    
    ÈáçË¶ÅÔºöËøô‰∏™ÂáΩÊï∞‰ºöÂú® subprocess ‰∏≠ËøêË°åÔºåCUDA_VISIBLE_DEVICES Â∑≤ÁªèÂú®ÂêØÂä®ÂâçËÆæÁΩÆ
    """
    # Âª∂ËøüÂØºÂÖ•ÊâÄÊúâÂèØËÉΩËß¶Âèë CUDA ÁöÑÂ∫ì
    import torch
    from PIL import Image
    import numpy as np
    from safetensors.torch import save_file
    from diffusers import AutoencoderKL
    
    device = torch.device("cuda:0")  # Âè™ËÉΩÁúãÂà∞‰∏ÄÂº†Âç°
    dtype = torch.bfloat16
    
    # Âä†ËΩΩ VAE
    print(f"[GPU {gpu_id}] Loading VAE...", flush=True)
    if os.path.isdir(vae_path):
        vae = AutoencoderKL.from_pretrained(vae_path, torch_dtype=dtype)
    elif vae_path.endswith(".safetensors"):
        vae = AutoencoderKL.from_single_file(vae_path, torch_dtype=dtype)
    else:
        raise ValueError(f"Unsupported VAE path: {vae_path}")
    
    vae.to(device)
    vae.eval()
    vae.requires_grad_(False)
    print(f"[GPU {gpu_id}] VAE loaded, processing {len(image_paths)} images", flush=True)
    
    output_path = Path(output_dir)
    input_root = Path(input_dir)
    processed = 0
    
    for i, img_path_str in enumerate(image_paths):
        image_path = Path(img_path_str)
        name = image_path.stem
        
        # Ê£ÄÊü•ÊòØÂê¶Â∑≤Â≠òÂú®
        existing = list(output_path.glob(f"{name}_*_{ARCHITECTURE}.safetensors"))
        if skip_existing and existing:
            progress_queue.put(("skip", gpu_id, 1))
            continue
        
        try:
            # Âä†ËΩΩÂõæÁâá
            image = Image.open(image_path).convert('RGB')
            w, h = image.size
            
            # Ë∞ÉÊï¥Â§ßÂ∞è
            aspect = w / h
            if aspect > 1:
                new_w = resolution
                new_h = int(resolution / aspect)
            else:
                new_h = resolution
                new_w = int(resolution * aspect)
            
            new_w = (new_w // 8) * 8
            new_h = (new_h // 8) * 8
            new_w = min(new_w, w)
            new_h = min(new_h, h)
            new_w = (new_w // 8) * 8
            new_h = (new_h // 8) * 8
            
            if (new_w, new_h) != (w, h):
                image = image.resize((new_w, new_h), Image.LANCZOS)
            
            w, h = image.size
            
            # ËΩ¨Êç¢‰∏∫ tensor
            img_array = np.array(image).astype(np.float32) / 255.0
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
            img_tensor = img_tensor * 2.0 - 1.0
            img_tensor = img_tensor.to(device=device, dtype=dtype)
            
            # ÁºñÁ†Å
            with torch.no_grad():
                latent = vae.encode(img_tensor).latent_dist.sample()
            
            # Â∫îÁî® scaling Âíå shift
            scaling_factor = getattr(vae.config, 'scaling_factor', 0.3611)
            shift_factor = getattr(vae.config, 'shift_factor', 0.1159)
            latent = (latent - shift_factor) * scaling_factor
            
            # ‰øùÂ≠ò
            latent = latent.cpu()
            F, H, W = 1, latent.shape[2], latent.shape[3]
            dtype_str = "bf16"
            
            # ËÆ°ÁÆóËæìÂá∫Ë∑ØÂæÑ
            try:
                rel_path = image_path.relative_to(input_root)
                target_dir = output_path / rel_path.parent
            except ValueError:
                target_dir = output_path
            
            target_dir.mkdir(parents=True, exist_ok=True)
            output_file = target_dir / f"{name}_{w}x{h}_{ARCHITECTURE}.safetensors"
            
            sd = {f"latents_{F}x{H}x{W}_{dtype_str}": latent.squeeze(0)}
            save_file(sd, str(output_file))
            
            processed += 1
            progress_queue.put(("done", gpu_id, 1))
            
        except Exception as e:
            print(f"[GPU {gpu_id}] Error: {image_path.name}: {e}", flush=True)
            progress_queue.put(("error", gpu_id, 1))
    
    # Ê∏ÖÁêÜ
    del vae
    import gc
    gc.collect()
    torch.cuda.empty_cache()
    
    progress_queue.put(("finished", gpu_id, processed))


def spawn_gpu_worker(gpu_id: int, vae_path: str, input_dir: str, output_dir: str,
                     resolution: int, skip_existing: bool, image_paths: List[str],
                     progress_queue):
    """
    Âú®ËÆæÁΩÆ CUDA_VISIBLE_DEVICES Âêé spawn ‰∏Ä‰∏™ worker Â≠êËøõÁ®ã
    """
    import multiprocessing as mp
    
    # ÂàõÂª∫Â≠êËøõÁ®ãÔºåÂú®Â≠êËøõÁ®ã fork/spawn ‰πãÂâçËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè
    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    
    # ‰ΩøÁî® subprocess ÂêØÂä®Áã¨Á´ã Python ËøõÁ®ãÔºàÊúÄÂÆâÂÖ®ÁöÑÊñπÂºèÔºâ
    import subprocess
    import json
    import tempfile
    
    # Â∞ÜÂèÇÊï∞ÂÜôÂÖ•‰∏¥Êó∂Êñá‰ª∂
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        params = {
            "gpu_id": gpu_id,
            "vae_path": vae_path,
            "input_dir": input_dir,
            "output_dir": output_dir,
            "resolution": resolution,
            "skip_existing": skip_existing,
            "image_paths": [str(p) for p in image_paths]
        }
        json.dump(params, f)
        params_file = f.name
    
    # ÊûÑÂª∫ÂÜÖËÅî worker ËÑöÊú¨
    worker_script = f'''
import os
import sys
import json

# ËÆæÁΩÆ CUDA_VISIBLE_DEVICESÔºàÂøÖÈ°ªÂú® import torch ‰πãÂâçÔºâ
os.environ["CUDA_VISIBLE_DEVICES"] = "{gpu_id}"

# Áé∞Âú®ÂèØ‰ª•ÂÆâÂÖ®ÂØºÂÖ•
import torch
from PIL import Image
import numpy as np
from safetensors.torch import save_file
from diffusers import AutoencoderKL
from pathlib import Path

ARCHITECTURE = "zi"

# ËØªÂèñÂèÇÊï∞
with open(r"{params_file}", "r") as f:
    params = json.load(f)

gpu_id = params["gpu_id"]
vae_path = params["vae_path"]
input_dir = params["input_dir"]
output_dir = params["output_dir"]
resolution = params["resolution"]
skip_existing = params["skip_existing"]
image_paths = params["image_paths"]

device = torch.device("cuda:0")
dtype = torch.bfloat16

# Âä†ËΩΩ VAE
print(f"[GPU {{gpu_id}}] Loading VAE...", flush=True)
if os.path.isdir(vae_path):
    vae = AutoencoderKL.from_pretrained(vae_path, torch_dtype=dtype)
elif vae_path.endswith(".safetensors"):
    vae = AutoencoderKL.from_single_file(vae_path, torch_dtype=dtype)
else:
    raise ValueError(f"Unsupported VAE path: {{vae_path}}")

vae.to(device)
vae.eval()
vae.requires_grad_(False)
print(f"[GPU {{gpu_id}}] VAE loaded, processing {{len(image_paths)}} images", flush=True)

output_path = Path(output_dir)
input_root = Path(input_dir)
processed = 0
total = len(image_paths)

for i, img_path_str in enumerate(image_paths):
    image_path = Path(img_path_str)
    name = image_path.stem
    
    # Ê£ÄÊü•ÊòØÂê¶Â∑≤Â≠òÂú®
    existing = list(output_path.glob(f"{{name}}_*_{{ARCHITECTURE}}.safetensors"))
    if skip_existing and existing:
        continue
    
    try:
        # Âä†ËΩΩÂõæÁâá
        image = Image.open(image_path).convert("RGB")
        w, h = image.size
        
        # Ë∞ÉÊï¥Â§ßÂ∞è
        aspect = w / h
        if aspect > 1:
            new_w = resolution
            new_h = int(resolution / aspect)
        else:
            new_h = resolution
            new_w = int(resolution * aspect)
        
        new_w = (new_w // 8) * 8
        new_h = (new_h // 8) * 8
        new_w = min(new_w, w)
        new_h = min(new_h, h)
        new_w = (new_w // 8) * 8
        new_h = (new_h // 8) * 8
        
        if (new_w, new_h) != (w, h):
            image = image.resize((new_w, new_h), Image.LANCZOS)
        
        w, h = image.size
        
        # ËΩ¨Êç¢‰∏∫ tensor
        img_array = np.array(image).astype(np.float32) / 255.0
        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
        img_tensor = img_tensor * 2.0 - 1.0
        img_tensor = img_tensor.to(device=device, dtype=dtype)
        
        # ÁºñÁ†Å
        with torch.no_grad():
            latent = vae.encode(img_tensor).latent_dist.sample()
        
        # Â∫îÁî® scaling Âíå shift
        scaling_factor = getattr(vae.config, "scaling_factor", 0.3611)
        shift_factor = getattr(vae.config, "shift_factor", 0.1159)
        latent = (latent - shift_factor) * scaling_factor
        
        # ‰øùÂ≠ò
        latent = latent.cpu()
        F, H, W = 1, latent.shape[2], latent.shape[3]
        dtype_str = "bf16"
        
        # ËÆ°ÁÆóËæìÂá∫Ë∑ØÂæÑ
        try:
            rel_path = image_path.relative_to(input_root)
            target_dir = output_path / rel_path.parent
        except ValueError:
            target_dir = output_path
        
        target_dir.mkdir(parents=True, exist_ok=True)
        output_file = target_dir / f"{{name}}_{{w}}x{{h}}_{{ARCHITECTURE}}.safetensors"
        
        sd = {{f"latents_{{F}}x{{H}}x{{W}}_{{dtype_str}}": latent.squeeze(0)}}
        save_file(sd, str(output_file))
        
        processed += 1
        
    except Exception as e:
        print(f"[GPU {{gpu_id}}] Error: {{image_path.name}}: {{e}}", flush=True)

# Ê∏ÖÁêÜ
del vae
import gc
gc.collect()
torch.cuda.empty_cache()

print(f"[GPU {{gpu_id}}] Completed: {{processed}} images", flush=True)

# Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂
os.remove(r"{params_file}")
'''
    
    # ÂêØÂä®Â≠êËøõÁ®ã
    process = subprocess.Popen(
        [sys.executable, "-c", worker_script],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        env=env,
        text=True,
        bufsize=1
    )
    
    return process


def main():
    parser = argparse.ArgumentParser(description="Cache latents for Z-Image training (Multi-GPU Safe)")
    parser.add_argument("--vae", type=str, required=True, help="VAE model path")
    parser.add_argument("--input_dir", type=str, required=True, help="Input image directory")
    parser.add_argument("--output_dir", type=str, required=True, help="Output cache directory")
    parser.add_argument("--resolution", type=int, default=1024, help="Target resolution")
    parser.add_argument("--batch_size", type=int, default=1, help="Batch size")
    parser.add_argument("--skip_existing", action="store_true", help="Skip existing cache files")
    parser.add_argument("--num_gpus", type=int, default=0, help="Number of GPUs (0=auto detect)")
    
    args = parser.parse_args()
    
    # ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Êü•ÊâæÂõæÁâá
    images = find_images(args.input_dir)
    total = len(images)
    print(f"Found {total} images", flush=True)
    
    if total == 0:
        print("No images to process", flush=True)
        return
    
    # Ê£ÄÊµã GPU Êï∞ÈáèÔºàÈÅøÂÖçÂú®‰∏ªËøõÁ®ãÂàùÂßãÂåñ CUDAÔºâ
    if args.num_gpus > 0:
        num_gpus = args.num_gpus
    else:
        try:
            import subprocess
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'],
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                num_gpus = len(result.stdout.strip().split('\n'))
            else:
                num_gpus = 1
        except Exception:
            num_gpus = 1
    
    if num_gpus <= 1:
        # Âçï GPU Ê®°Âºè
        import torch
        from PIL import Image
        import numpy as np
        from safetensors.torch import save_file
        from diffusers import AutoencoderKL
        
        print(f"Using single GPU mode", flush=True)
        print(f"Progress: 0/{total}", flush=True)
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        dtype = torch.bfloat16
        
        print(f"Loading VAE: {args.vae}", flush=True)
        if os.path.isdir(args.vae):
            vae = AutoencoderKL.from_pretrained(args.vae, torch_dtype=dtype)
        else:
            vae = AutoencoderKL.from_single_file(args.vae, torch_dtype=dtype)
        vae.to(device)
        vae.eval()
        vae.requires_grad_(False)
        print("VAE loaded successfully", flush=True)
        
        processed = 0
        skipped = 0
        input_root = Path(args.input_dir)
        
        for i, image_path in enumerate(images, 1):
            name = image_path.stem
            existing = list(output_dir.glob(f"{name}_*_{ARCHITECTURE}.safetensors"))
            if args.skip_existing and existing:
                skipped += 1
                print(f"Progress: {i}/{total}", flush=True)
                continue
            
            try:
                image = Image.open(image_path).convert('RGB')
                w, h = image.size
                
                aspect = w / h
                if aspect > 1:
                    new_w = args.resolution
                    new_h = int(args.resolution / aspect)
                else:
                    new_h = args.resolution
                    new_w = int(args.resolution * aspect)
                
                new_w = (new_w // 8) * 8
                new_h = (new_h // 8) * 8
                new_w = min(new_w, w)
                new_h = min(new_h, h)
                new_w = (new_w // 8) * 8
                new_h = (new_h // 8) * 8
                
                if (new_w, new_h) != (w, h):
                    image = image.resize((new_w, new_h), Image.LANCZOS)
                
                w, h = image.size
                
                img_array = np.array(image).astype(np.float32) / 255.0
                img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
                img_tensor = img_tensor * 2.0 - 1.0
                img_tensor = img_tensor.to(device=device, dtype=dtype)
                
                with torch.no_grad():
                    latent = vae.encode(img_tensor).latent_dist.sample()
                
                scaling_factor = getattr(vae.config, 'scaling_factor', 0.3611)
                shift_factor = getattr(vae.config, 'shift_factor', 0.1159)
                latent = (latent - shift_factor) * scaling_factor
                
                latent = latent.cpu()
                F, H, W = 1, latent.shape[2], latent.shape[3]
                
                try:
                    rel_path = image_path.relative_to(input_root)
                    target_dir = output_dir / rel_path.parent
                except ValueError:
                    target_dir = output_dir
                
                target_dir.mkdir(parents=True, exist_ok=True)
                output_file = target_dir / f"{name}_{w}x{h}_{ARCHITECTURE}.safetensors"
                
                sd = {f"latents_{F}x{H}x{W}_bf16": latent.squeeze(0)}
                save_file(sd, str(output_file))
                
                processed += 1
                print(f"Progress: {i}/{total}", flush=True)
            except Exception as e:
                print(f"Error: {image_path}: {e}", flush=True)
                print(f"Progress: {i}/{total}", flush=True)
        
        print(f"Latent caching completed! Processed: {processed}, Skipped: {skipped}", flush=True)
        
        del vae
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("VAE unloaded, GPU memory released", flush=True)
    
    else:
        # Â§ö GPU Ê®°Âºè - ‰ΩøÁî® subprocess ÂêØÂä®Áã¨Á´ãËøõÁ®ã
        print(f"üöÄ Multi-GPU mode: using {num_gpus} GPUs", flush=True)
        print(f"Progress: 0/{total}", flush=True)
        
        # ÂàÜÁâá
        chunk_size = (total + num_gpus - 1) // num_gpus
        chunks = []
        for i in range(num_gpus):
            start = i * chunk_size
            end = min(start + chunk_size, total)
            if start < total:
                chunks.append((i, [str(p) for p in images[start:end]]))
        
        print(f"Distributing {total} images across {len(chunks)} GPUs", flush=True)
        for gpu_id, chunk in chunks:
            print(f"  GPU {gpu_id}: {len(chunk)} images", flush=True)
        
        # ÂêØÂä®ÊâÄÊúâ worker ËøõÁ®ã
        processes = []
        for gpu_id, image_paths in chunks:
            p = spawn_gpu_worker(
                gpu_id, args.vae, args.input_dir, args.output_dir,
                args.resolution, args.skip_existing, image_paths, None
            )
            processes.append((gpu_id, p))
        
        # Êî∂ÈõÜËæìÂá∫Âπ∂Á≠âÂæÖÂÆåÊàê
        import threading
        import queue
        
        output_queue = queue.Queue()
        
        def read_output(gpu_id, process, q):
            for line in process.stdout:
                q.put((gpu_id, line.rstrip()))
            process.wait()
        
        threads = []
        for gpu_id, p in processes:
            t = threading.Thread(target=read_output, args=(gpu_id, p, output_queue), daemon=True)
            t.start()
            threads.append(t)
        
        # ËæìÂá∫ËøõÂ∫¶
        completed = 0
        progress_count = 0
        while completed < len(processes):
            try:
                gpu_id, line = output_queue.get(timeout=0.1)
                print(f"[GPU {gpu_id}] {line}", flush=True)
                if "Completed:" in line:
                    completed += 1
                elif not line.startswith("[GPU"):
                    # ÂèØËÉΩÊòØËøõÂ∫¶‰ø°ÊÅØ
                    progress_count += 1
                    if progress_count % 10 == 0:
                        print(f"Progress: {progress_count}/{total}", flush=True)
            except queue.Empty:
                # Ê£ÄÊü•ËøõÁ®ãÊòØÂê¶ÁªìÊùü
                all_done = all(p.poll() is not None for _, p in processes)
                if all_done:
                    break
        
        # Á≠âÂæÖÊâÄÊúâÁ∫øÁ®ãÁªìÊùü
        for t in threads:
            t.join(timeout=1)
        
        print(f"Multi-GPU latent caching completed!", flush=True)


if __name__ == "__main__":
    main()
